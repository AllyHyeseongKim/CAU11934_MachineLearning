{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment05.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1J17l_shxZiTkzFM7K-V5Ww_ENOtj8_db",
      "authorship_tag": "ABX9TyM9TRz2KNCMw2s50suUSofn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllyHyeseongKim/CAU11934_MachineLarning/blob/master/assignment/05/assignment05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GuLdkxaa4i_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment04: Logistic regression for a binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73H4LqObBJt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the input data (text file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLerC-j0bS6y",
        "colab_type": "text"
      },
      "source": [
        "### Mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jbzi_q78OI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2KfvSmXbgyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd gdrive/My Drive/Colab Notebooks/Machine Learning/assignment05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3l-qonsbhly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhB0qrLob9vL",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "Load a set of the data $\\{ (x^{(i)}, y^{(i)}, l^{(i)}) \\}$ from the given `text file` (`'data.txt'`) for training. \\\\\n",
        "Each row $\\{ (x^{(i)}, y^{(i)}, l^{(i)}) \\}$ of the data consists of a 2-dimensional point $(x, y)$ with its label $l$, $\\text{where $x, y \\in \\mathbb{R}$ and $l \\in \\mathbb{\\{0, 1\\}}$}$. \\\\\n",
        "Print the set of points $\\{ (x^{(i)}, y^{(i)}) \\}$ that are loaded from `'data_train.csv'` file. \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnORd4lSd0Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data    = np.genfromtxt(\"data.txt\", delimiter=',', dtype=np.float64)\n",
        "\n",
        "x_train = data[:, 0]\n",
        "y_train = data[:, 1]\n",
        "label   = data[:, 2]\n",
        "\n",
        "m = len(x_train)\n",
        "\n",
        "x_label0    = x_train[label == 0]\n",
        "x_label1    = x_train[label == 1]\n",
        "\n",
        "y_label0    = y_train[label == 0]\n",
        "y_label1    = y_train[label == 1]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(x_label0, y_label0, alpha=0.3, c='b')\n",
        "plt.scatter(x_label1, y_label1, alpha=0.3, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICZg-SmnebB0",
        "colab_type": "text"
      },
      "source": [
        "## 2. Generate the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfh2gOoOeins",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `logistic regression`\n",
        "\n",
        "Define the following `Sigmoid function`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{h} = \\sigma(z) \\\\\n",
        "z = \\theta_0 + \\theta_1x + \\theta_2y, \\quad\\text{where $\\theta_0, \\theta_1, \\theta_2 \\in \\mathbb{R}$} \\\\\n",
        "\\sigma(z) = \\frac{1}{1 + exp(-z)} \\\\\n",
        "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU8EcV4Ty_52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic_regression(weight, offset):\n",
        "    z = []\n",
        "    for i in range(m):\n",
        "        z.append(offset + weight[0] * x_train[i] + weight[1] * y_train[i])\n",
        "    y_logistic_regression = []\n",
        "    for i in range(m):\n",
        "        y_logistic_regression.append(1/(1 + np.exp(-z[i])))\n",
        "        # print(y_logistic_regression[i])        \n",
        "    return y_logistic_regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b8aZyWbf6Y2",
        "colab_type": "text"
      },
      "source": [
        "## 3. Generate the `Cost Function` with `Gradient Descent` method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45fI63vYgDHI",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `objective function`\n",
        "\n",
        "Define the following `objective function`.\n",
        "\n",
        "\\begin{equation*}\n",
        "J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{m}\\sum_{i = 1}^m(-l^{(i)}log(\\sigma(z^{(i)})) - (1 - l^{(i)})log(1 - \\sigma(z^{(i)})))\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHcuTrNo2d2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective_function(y_logistic_regression):\n",
        "    error = []\n",
        "    for i in range(m):\n",
        "        error.append((-label[i]) * np.log(y_logistic_regression[i]) - (1 - label[i]) * np.log(1 - y_logistic_regression[i]))\n",
        "    return sum(error) / m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBIcpINBgcWk",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `gradient descent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsIk54zBgfUl",
        "colab_type": "text"
      },
      "source": [
        "Define the following `gradient descent`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\theta_0^{(t+1)} := \\theta_0^{(t)} - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(\\sigma(z^{(i)}) - l^{(i)}) \\\\\n",
        "\\theta_1^{(t+1)} := \\theta_1^{(t)} - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(\\sigma(z^{(i)}) - l^{(i)})x^{(i)} \\\\\n",
        "\\theta_2^{(t+1)} := \\theta_2^{(t)} - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(\\sigma(z^{(i)}) - l^{(i)})y^{(i)} \\\\\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nGwBp-3g8r4",
        "colab_type": "text"
      },
      "source": [
        "Define the `learning rate`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\alpha  = 0.000012\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwEBNNh_4BZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxBx0Vhj4D2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(weight, offset, y_logistic_regression):\n",
        "    offset_error = []\n",
        "    weightPrime = []\n",
        "    for i in range(m):\n",
        "        offset_error.append(y_logistic_regression[i] - label[i])\n",
        "    offsetPrime = offset - learning_rate * sum(offset_error) / m\n",
        "    weight_error = []\n",
        "    weight_error.append([])\n",
        "    weight_error.append([])\n",
        "    for i in range(m):\n",
        "        weight_error[0].append((y_logistic_regression[i] - label[i]) * x_train[i])\n",
        "        weight_error[1].append((y_logistic_regression[i] - label[i]) * y_train[i])\n",
        "    weightPrime.append(weight[0] - learning_rate * sum(weight_error[0]) / m)\n",
        "    weightPrime.append(weight[1] - learning_rate * sum(weight_error[1]) / m)\n",
        "    return weightPrime, offsetPrime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avzBU42whChm",
        "colab_type": "text"
      },
      "source": [
        "## 4. `Train` the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc29_kjVhFv_",
        "colab_type": "text"
      },
      "source": [
        "Define the initial `weight`$(\\theta_1^{(0)}, \\theta_2^{(0)})$ and `offset`$(\\theta_0^{(0)})$:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\theta_0^{(0)} = 1, \\theta_1^{(0)} = 1, \\theta_2^{(0)}  = 1.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps0Pw9yfDiM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = []\n",
        "weight.append([])\n",
        "offset = []\n",
        "weight[0].append(-1)\n",
        "weight[0].append(0)\n",
        "offset.append(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GouNdNYwDmhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost_convergence = []\n",
        "theta_convergence = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpaOkRUohnat",
        "colab_type": "text"
      },
      "source": [
        "`Train` the `input data` with the `logistic regression` function above with the `gradient descent`. \\\\\n",
        "Find optimal parameters $(\\theta_0, \\theta_1, \\theta_2)$ using the `traing data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26waG-yhDqYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = m * 10\n",
        "sigma = []\n",
        "j = []\n",
        "i = 0\n",
        "# print(logistic_regression(weight[0], offset[0]))\n",
        "# print(objective_function(logistic_regression(weight[0], offset[0])))\n",
        "sigma.append(logistic_regression(weight[i], offset[i]))\n",
        "j.append(objective_function(logistic_regression(weight[i], offset[i])))\n",
        "# print(j)\n",
        "\n",
        "while i < epoch:\n",
        "    i = i + 1\n",
        "\n",
        "#    print(i)\n",
        "\n",
        "    weight.append(gradient_descent(weight[i - 1], offset[i - 1], logistic_regression(weight[i - 1], offset[i - 1]))[0])\n",
        "    offset.append(gradient_descent(weight[i - 1], offset[i - 1], logistic_regression(weight[i - 1], offset[i - 1]))[1])\n",
        "\n",
        "#    print('weight: ', weight)\n",
        "#    print('offset: ', offset)\n",
        "\n",
        "    sigma.append(logistic_regression(weight[i], offset[i]))\n",
        "#    print('sigma: ', sigma)\n",
        "#    print(offset[i])\n",
        "#    print(round(j[i - 1], 2))\n",
        "    j.append(objective_function(logistic_regression(weight[i], offset[i])))\n",
        "#    print('j: ', j)\n",
        "    if (j[i] == j[i - 1]):\n",
        "        cost_convergence.append(i)\n",
        "    if round(weight[i][0], 4) == round(weight[i - 1][0], 4):\n",
        "        if round(weight[i][1], 4) == round(weight[i - 1][1], 4):\n",
        "            if round(offset[i], 4) == round(offset[i - 1], 4):\n",
        "#                print(i)\n",
        "                theta_convergence.append(i)\n",
        "\n",
        "print(theta_convergence)\n",
        "# print('sigma: ', sigma)\n",
        "print('j: ', j)\n",
        "print('weight: ', weight)\n",
        "print('offset: ', offset)\n",
        "\n",
        "x_out = np.arange(0, epoch + 1)\n",
        "plt.plot(x_out, j, color = 'blue')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLlVemvjh_Lh",
        "colab_type": "text"
      },
      "source": [
        "## 5. Visualize the `Classifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYOdbuUYiJZx",
        "colab_type": "text"
      },
      "source": [
        "Visualize the obtained `classifier` with varying $x$ and $y$ values that range `from the minimum to the maximum` of the `training data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81-m_hsLiaD4",
        "colab_type": "text"
      },
      "source": [
        "## 6. **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVKddeVniey8",
        "colab_type": "text"
      },
      "source": [
        "### 1. **Plot the training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOx5LpA0i2EE",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training data points` $(x, y)$ with their `labels` $l$ (in `blue` color for `label 0` and `red` color for `label 1`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeDalXKGikYf",
        "colab_type": "text"
      },
      "source": [
        "### 2. **Plot the estimated parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-pt65qejLnv",
        "colab_type": "text"
      },
      "source": [
        "Plot the `estimated parameters` $(\\theta_0, \\theta_1, \\theta_2)$ at `every iteration` of `gradient descent` until `convergence` (in `red`, `green`, `blue` color, respectively).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snzDttg-ioKB",
        "colab_type": "text"
      },
      "source": [
        "###3. **Plot the training error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLMm1o26jbzc",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training error` $J(\\theta_0, \\theta_1, \\theta_2)$ at `every iteration` of `gradient descent` until `convergence` (in `blue` color)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp6W-ytRisIZ",
        "colab_type": "text"
      },
      "source": [
        "### 4. **Plot the obtained classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnpoCc9WjjrF",
        "colab_type": "text"
      },
      "source": [
        "Plot the `classifier` \n",
        "\\begin{equation*}\n",
        "\\sigma(z), \\quad \\text{where $z = \\theta_0 + \\theta_1 x + \\theta_2 y$, with $x = [30 : 0.5 : 100]$, and $y = [30 : 0.5 : 100]$}, \\quad \\text{where $[a : t : b]$ denotes a range of values from $a$ to $b$ with a stepsize $t$}.\n",
        "\\end{equation*}\n",
        "Use a `colormap` where `blue` is used for `0`, `red` is used for `1`, their `weighted combination` for a value `between 0 and 1`. \\\\\n",
        "Plot the `training data points` $(x, y)$ with their `labels` $l$ (in `blue` color for `label 0` and `red` color for `label 1`) `superimposed` on the `classifier`.\n"
      ]
    }
  ]
}