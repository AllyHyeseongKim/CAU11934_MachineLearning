{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment11",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO+iRtdk5S+7R0L4Y/OJmQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllyHyeseongKim/CAU11934_MachineLearning/blob/feature%2Fassignment11/assignment/11/assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUL5PMpo3xb0",
        "colab_type": "text"
      },
      "source": [
        "# Assignment11: Text classification using neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBIU9KR33Y9",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the input data (txt file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wid6MPJ735VJ",
        "colab_type": "text"
      },
      "source": [
        "### Mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxDJ0-24xu0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFxBfptArts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Cccwg2rv9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd ../content/gdrive/My Drive/Colab Notebooks/Machine Learning/assignment11"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeGW-jTN3_3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9WsauyO4IAX",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "Load a set of the data from the given `data directory` (`'movie_review'`) consists of two sub-directories (`'pos'`) and (`'neg'`) for `positive` and `negative`, respectively. \\\\\n",
        "Each sub-directory includes a `list of files` for `review texts`. \\\\\n",
        "The `preprocessing` transforms each `text` into the `frequency information`. \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5FKiaa-kqE",
        "colab_type": "text"
      },
      "source": [
        "The data `preprocessing` steps aim to transform `text data` into `informative quantity` with respect to the class. \\\\\n",
        "It is allowed to use any embedding scheme to transform `text data` into `descriptors` using any libraries. \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dABXry9Br0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cupy as cp\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS08wF9rCyV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = cp.array(X_train), cp.array(X_test), cp.array(y_train), cp.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2dqXYEPjHFr",
        "colab_type": "text"
      },
      "source": [
        "## 2. Neural Network Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjN8pxYcjVMf",
        "colab_type": "text"
      },
      "source": [
        "```mermaid\n",
        "(input layer : x)  --> (first hidden layer : y)  -->  (output layer : h)\n",
        "```\n",
        "\n",
        "```mermaid\n",
        "(x)  -- fully connected : u -->  (y_)  -- sigmoid -->  (y)  -- fully connected : v -->  (h_)  -- sigmoid -->  (h)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX8wzzKLPcAO",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Generate the Fully Connected Layer\n",
        "\n",
        "Define the following `fully connected layer` with a `bias`.\n",
        "\n",
        "\\begin{equation*}\n",
        "(output \\ layer) = 1\\times\\theta_0^t + (input \\ layer)_1\\times\\theta_1^t + (input \\ layer)_2\\times\\theta_2^t + ... + (input \\ layer)_{num \\ input}^t, \\quad\\text{where, $t =$ (the number of the iteration of the layer)}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodvsIriCLBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fully_connected(num_input, num_output, weight, input_layer, num_image):\n",
        "    output_layer  = cp.empty((num_image, num_output), dtype=float)\n",
        "    input_reshaped = cp.ones((num_input + 1, num_image), dtype=float)\n",
        "    input_reshaped[1:] = input_layer\n",
        "    weight_reshaped = weight.reshape(num_output, num_input + 1)\n",
        "    output_layer = cp.matmul(weight_reshaped, input_reshaped)\n",
        "    return output_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5HbqtQ5U9C",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Generate the Sigmoid Function as an Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFozuJ3e5Yi_",
        "colab_type": "text"
      },
      "source": [
        "#### Generate the `sigmoid function`\n",
        "\n",
        "Define the following `sigmoid fuction` as an `activation fuction`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sigma(z) = \\frac{1}{1 + exp(-z)} \\\\\n",
        "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\\\\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF_EH1XyYihP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(input_layer, num_image):\n",
        "    matrix_ones = cp.ones_like(input_layer)\n",
        "    output_layer  = cp.reciprocal(cp.add(matrix_ones, cp.exp(cp.negative(input_layer))))\n",
        "    return output_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dYkIe1CwwP",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Generate the `Objective Function`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bSkzSTtH6hz",
        "colab_type": "text"
      },
      "source": [
        "Define the `regularization parameter`.\n",
        "\n",
        "\\begin{equation}\n",
        "\\lambda = 0.5\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv-dWIhoJcg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regularization_parameter = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PIj7i3vDSvN",
        "colab_type": "text"
      },
      "source": [
        "Define the following `objective function`.\n",
        "\n",
        "\\begin{equation*}\n",
        "J(\\theta) = \\frac{1}{m}​\\sum_{i = 1}^m​\\sum_{k = 0}^9​(−l_k^{(i)}​log(h_k^{(i)}​)−(1−l_k^{(i)}​)log(1−h_k^{(i)}​)) + \\frac{\\lambda}{2n}\\sum_{j = 1}^n\\theta_j^2, \\\\\n",
        "\\text{where,}\\quad \\theta_j \\text{ denotes a model parameter where $j = 1, 2, ..., n$}, \\theta = (u, v), \\\\\n",
        "\\lambda \\text{ is a control parameter for the regularization based on the $L_2^2$-norm (weight decay)}, \\\\\n",
        "n\\text{ is the total number of all the model parameters over the entire neural network}, \\\\\n",
        "\\text{ and $h_k^{(i)}$ denotes the $k^{th}$ element of the output layer for $i^{th}$ sample data.}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJoGKM5hfzyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(output_layer, weight1, weight2, num_image, label):\n",
        "    matrix_ones = cp.ones_like(output_layer)\n",
        "    regularization = (cp.sum(cp.power(weight1, 2)) + cp.sum(cp.power(weight2, 2))) * regularization_parameter / (2 * n)\n",
        "    loss = cp.sum(cp.subtract(cp.multiply(label, cp.sum(cp.log(cp.subtract(cp.reciprocal(output_layer), matrix_ones)), axis=0)), cp.sum(cp.log(cp.subtract(matrix_ones, output_layer)), axis=0))) / num_image\n",
        "    loss = loss + regularization\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirfXRGhEcYF",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Generate the `Gradient Descent` (`Back-Propagation`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa7zkkL1DPfF",
        "colab_type": "text"
      },
      "source": [
        "Define the `learning rate`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\alpha  = 0.0001\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFCrlBMrCLZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyybBUmEz2W",
        "colab_type": "text"
      },
      "source": [
        "Define the following `gradient descent`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\theta_k^{(t + 1)} := \\theta_k^{(t)} - \\alpha\\frac{\\partial J(\\theta^{(t)})}{\\partial \\theta_k}, \\quad\\text{for all $k$}.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJHUNfKu3lG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent2(weight):\n",
        "    num_y = len(train_hidden_layer_y)\n",
        "    num_h = len(train_output_layer_h)\n",
        "    weight_reshaped = weight.reshape(num_h, num_y + 1)\n",
        "    y_reshaped = cp.ones((num_y + 1, num_train_image), dtype=float)\n",
        "    y_reshaped[1:] = train_hidden_layer_y\n",
        "    matrix_partial_loss = cp.matmul(cp.subtract(train_output_layer_h, cp.tile(list_label_train, (num_h, 1))), cp.transpose(y_reshaped)) / num_train_image\n",
        "    matrix_partial_loss = matrix_partial_loss + weight_reshaped * (regularization_parameter / n)\n",
        "    weight_update = cp.subtract(weight_reshaped, learning_rate * matrix_partial_loss)\n",
        "    return weight_update.reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MH74K2dpXt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent1(weight_u, weight_v):\n",
        "    num_x = len(train_input_layer_x)\n",
        "    num_y = len(train_hidden_layer_y)\n",
        "    num_h = len(train_output_layer_h)\n",
        "    weight_reshaped = weight_v.reshape(num_y, num_x + 1)\n",
        "    x_reshaped = cp.ones((num_x + 1, num_train_image), dtype=float)\n",
        "    x_reshaped[1:] = train_input_layer_x\n",
        "    y_reshaped = cp.multiply(train_hidden_layer_y, 1 - train_hidden_layer_y)\n",
        "    matrix_partial_loss = cp.matmul(cp.transpose(cp.matmul(cp.transpose(cp.subtract(train_output_layer_h, cp.tile(list_label_train, (num_h, 1)))), weight_u.reshape(num_h, num_y + 1)[:, 1:])) * y_reshaped, cp.transpose(x_reshaped)) / num_train_image\n",
        "    matrix_partial_loss = matrix_partial_loss + weight_reshaped * (regularization_parameter / n)\n",
        "    weight_update = cp.subtract(weight_reshaped, learning_rate * matrix_partial_loss)\n",
        "    return weight_update.reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYzUFM4jq0Mx",
        "colab_type": "text"
      },
      "source": [
        "### 2.4. Compute the `Accuracy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX_1DMDZq898",
        "colab_type": "text"
      },
      "source": [
        "Compute the following `accuracy` in `number (%)`.\n",
        "\\begin{equation}\n",
        "accuracy\\ (\\%) = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}} \\times 100\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD0X0W0WrLYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output_layer, num_image, label):\n",
        "    num_correct_predict = cp.count_nonzero(cp.equal(cp.around(output_layer), label))\n",
        "    return (num_correct_predict / num_image) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQ5fNon_luS",
        "colab_type": "text"
      },
      "source": [
        "### 2.4. `Train` and `Test` the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmCtCARLAJiS",
        "colab_type": "text"
      },
      "source": [
        "Define the `initial conditions` of `weights` $(\\theta_{0}^{(0)}, \\theta_{1}^{(0)}, \\theta_{2}^{(0)}, ..., \\theta_{28\\times28}^{(0)})$. \\\\\n",
        "The `weights` ar following a `normal distribution` $\\mathcal{N}(0, \\sigma^2)$ with `mean` 0 and `standard deviation` some number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aw4kFwBGOgn",
        "colab_type": "text"
      },
      "source": [
        "Define the `standard deviation`. \\\\\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sigma = 0.1\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ttBMUdY9iXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = 0\n",
        "standard_deviation = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE29KmutnDDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP0O-QmmYih8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize(size):\n",
        "    weight = cp.random.normal(mean, standard_deviation, size)\n",
        "    return weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z63hYVN3VTGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight1 = cp.empty((epoch, 1501 * 36), dtype=float)\n",
        "weight1[0] = initialize(1501 * 36)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDvI8vJ8X9t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight2 = cp.empty((epoch, 37 * 1), dtype=float)\n",
        "weight2[0] = initialize(37 * 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oux9RIe6KDGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(weight1[0]) + len(weight2[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKT5lGy2A5V8",
        "colab_type": "text"
      },
      "source": [
        "`Train` the `train data` with the `Neural Network Architecture` above with the `gradient descent`. \\\\\n",
        "Find `optimal parameters` $\\theta$ using the `traing data` (the first `1,000 images`). \\\\\n",
        "`Test` the `test data` with the `Neural Network Architecture` above with the `obtained parameters` $\\theta$ from the `training process` using the `testing data` (the rest `9,000 images`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp9jnAk5lH8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_layer_x = cp.empty((1501, num_train_image), dtype=float)\n",
        "train_y = cp.empty((36, num_train_image), dtype=float)\n",
        "train_hidden_layer_y = cp.empty((36, num_train_image), dtype=float)\n",
        "train_h = cp.empty((1, num_train_image), dtype=float)\n",
        "train_output_layer_h = cp.empty((1, num_train_image), dtype=float)\n",
        "train_input_layer_x = list_image_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A3lchJ2pSx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_layer_x = cp.empty((1501, num_test_image), dtype=float)\n",
        "test_y = cp.empty((36, num_test_image), dtype=float)\n",
        "test_hidden_layer_y = cp.empty((36, num_test_image), dtype=float)\n",
        "test_h = cp.empty((1, num_test_image), dtype=float)\n",
        "test_output_layer_h = cp.empty((1, num_test_image), dtype=float)\n",
        "test_input_layer_x = list_image_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miiukdoxnSQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = np.empty(epoch, dtype=float)\n",
        "test_loss = np.empty(epoch, dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZpaLijfp21X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy = np.empty(epoch, dtype=float)\n",
        "test_accuracy = np.empty(epoch, dtype=np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3o2M_mmjz3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(epoch):\n",
        "    train_y = fully_connected(784, 16, weight1[i], train_input_layer_x, num_train_image)\n",
        "    train_hidden_layer_y = sigmoid(train_y, num_train_image)\n",
        "    train_h = fully_connected(16, 10, weight2[i], train_hidden_layer_y, num_train_image)\n",
        "    train_output_layer_h = sigmoid(train_h, num_train_image)\n",
        "\n",
        "    train_loss[i] = objective(train_output_layer_h, weight1[i], weight2[i], num_train_image, list_label_train)\n",
        "    train_accuracy[i] = accuracy(train_output_layer_h, num_train_image, list_label_train)\n",
        "\n",
        "    #print(\"[\", i + 1, \"/\", epoch, \"]\", \"train loss: \", train_loss[i], \", train accuracy: \", train_accuracy[i])\n",
        "\n",
        "    test_y = fully_connected(784, 16, weight1[i], test_input_layer_x, num_test_image)\n",
        "    test_hidden_layer_y = sigmoid(test_y, num_test_image)\n",
        "    test_h = fully_connected(16, 10, weight2[i], test_hidden_layer_y, num_test_image)\n",
        "    test_output_layer_h = sigmoid(test_h, num_test_image)\n",
        "\n",
        "    test_loss[i] = objective(test_output_layer_h, weight1[i], weight2[i], num_test_image, list_label_test)\n",
        "    test_accuracy[i] = accuracy(test_output_layer_h, num_test_image, list_label_test)\n",
        "\n",
        "    #print(\"[\", i + 1, \"/\", epoch, \"]\", \"test loss: \", test_loss[i], \", test accuracy: \", test_accuracy[i])\n",
        "\n",
        "    if i < (epoch - 1):\n",
        "        weight2[i + 1] = gradient_descent2(weight2[i])[0]\n",
        "        weight1[i + 1] = gradient_descent1(weight2[i], weight1[i])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fVy9RdHvofq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(list_label, list_image, i):\n",
        "    label       = str(list_label)\n",
        "    im_vector   = list_image\n",
        "    im_matrix   = im_vector.reshape((size_row, size_col))\n",
        "    \n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.title(label)\n",
        "    print(\"1\")\n",
        "    plt.imshow(im_matrix, cmap='Greys', interpolation='None')\n",
        "    print(\"2\")\n",
        "\n",
        "    frame   = plt.gca()\n",
        "    frame.axes.get_xaxis().set_visible(False)\n",
        "    frame.axes.get_yaxis().set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eS9x-OPzhbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_correct = []\n",
        "test_miss = []\n",
        "for i in range(num_test_image):\n",
        "    if np.argmax(test_output_layer_h[:, i]) == list_label_test[i]:\n",
        "        test_correct.append(i)\n",
        "    else:\n",
        "        test_miss.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7KprHVQDhY3",
        "colab_type": "text"
      },
      "source": [
        "## 3. **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqRkLfC5Dlpq",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. **Plot the loss curve**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ysZpDTDojS",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training loss` at `every iteration` of `gradient descent` using the `training data` (in `blue` color). \\\\\n",
        "Plot the `testing loss` at `every iteration` of `gradient descent` using the `testing data` (in `red` color). \\\\\n",
        "The both `curves` should be presented in `one figure`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyyRWqECQaxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "x_cost1 = np.arange(0, epoch)\n",
        "x_cost2 = np.arange(0, epoch)\n",
        "plt.xlabel('t (iteration)')\n",
        "plt.ylabel('J(theta)')\n",
        "\n",
        "plt.plot(x_cost1, train_loss[:epoch], color = 'blue', label = 'training loss')\n",
        "plt.plot(x_cost2, test_loss[:epoch], color = 'red', label = 'testing loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMDeilIRDuS2",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. **Plot the accuracy curve**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuqmpAX-w96y",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training accuracy` (%) at `every iteration` of `gradient descent` using the `training data` (in `blue` color). \\\\\n",
        "plot the `testing accuracy` (%) at `every iteration` of `gradient descent` using the `testing data` (in `red` color). \\\\\n",
        "The both `curves` should be presented in `one figure`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EpsSm9ow880",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.xlabel('t (iteration)')\n",
        "plt.ylabel('accuracy(%)')\n",
        "\n",
        "plt.plot(x_cost1, train_accuracy[:epoch], color = 'blue', label = 'training accuracy')\n",
        "plt.plot(x_cost2, test_accuracy[:epoch], color = 'red', label = 'testing accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADlDFzpL83CC",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. **Plot the quantitative results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLdK0PDL9Dgm",
        "colab_type": "text"
      },
      "source": [
        "#### `Training results`\n",
        "Print the `confusion matrix` using the `function confusion_matrix` based on the `training data`. \\\\\n",
        "Print the `classification report` using the `function classification_report` based on the `training data`. \\\\\n",
        "Print the `accuracy score` using the `function accuracy_score` based on the `training data`. \\\\\n",
        "\n",
        "\n",
        "#### `Testing results`\n",
        "\n",
        "Print the `confusion matrix` using the `function confusion_matrix` based on the testing data. \\\\\n",
        "Print the `classification report` using the `function classification_report` based on the `testing data`. \\\\\n",
        "Print the `accuracy score` using the `function accuracy_score` based on the `testing data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBP6Lc-h---a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Final training accuracy: \", train_accuracy[epoch - 1], \"%\")\n",
        "print(\"Final testing accuracy: \", test_accuracy[epoch - 1], \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHHdgMtLsNf",
        "colab_type": "text"
      },
      "source": [
        "### 3.4. **Testing accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiXRP5pt7R8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Final testing accuracy: \", test_accuracy[epoch - 1], \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}