{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment07.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOngjiUEUqonhsOjHCUn7AX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllyHyeseongKim/CAU11934_MachineLarning/blob/master/assignment/07/assignment07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUL5PMpo3xb0",
        "colab_type": "text"
      },
      "source": [
        "# Assignment07: Logistic regression for a binary classification with a regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBIU9KR33Y9",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the input data (text file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wid6MPJ735VJ",
        "colab_type": "text"
      },
      "source": [
        "### Mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxDJ0-24xu0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivRVVPKK387E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd gdrive/My Drive/Colab Notebooks/Machine Learning/assignment06"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeGW-jTN3_3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9WsauyO4IAX",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "Load a set of the data $\\{ (x^{(i)}, y^{(i)}, l^{(i)}) \\}$ from the given `text file` (`'data-nonlinear.txt'`) for training. \\\\\n",
        "Each row $\\{ (x^{(i)}, y^{(i)}, l^{(i)}) \\}$ of the data consists of a 2-dimensional point $(x, y)$ with its label $l$, $\\text{where $x, y \\in \\mathbb{R}$ and $l \\in \\{0, 1\\}$}$. \\\\\n",
        "Plot the set of points $\\{ (x^{(i)}, y^{(i)}) \\}$ that are loaded from `'data-nonlinear.csv'` file. \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VadCSLtn4h4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data    = np.genfromtxt(\"data-nonlinear.txt\", delimiter=',', dtype=np.float64)\n",
        "\n",
        "x_train  = data[:, 0]\n",
        "y_train  = data[:, 1]\n",
        "label   = data[:, 2]\n",
        "\n",
        "m = len(x_train)\n",
        "\n",
        "pointX0 = x_train[label == 0]\n",
        "pointY0 = y_train[label == 0]\n",
        "\n",
        "pointX1 = x_train[label == 1]\n",
        "pointY1 = y_train[label == 1]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(pointX0, pointY0, c='b')\n",
        "plt.scatter(pointX1, pointY1, c='r')\n",
        "plt.tight_layout()\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5HbqtQ5U9C",
        "colab_type": "text"
      },
      "source": [
        "## 2. Generate the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFozuJ3e5Yi_",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `logistic regression`\n",
        "\n",
        "Define the following `logistic regression`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{h} = \\sigma(z) \\\\\n",
        "z = g(x, y; \\theta), \\quad\\text{where $g$ is a high dimensional function and $\\theta \\in \\mathbb{R}^k$} \\\\\n",
        "\\theta = (\\theta_0, \\theta_1, ..., \\theta_{k-1}) \\\\\n",
        "g(x, y; \\theta) = \\theta_0f_0(x, y) + \\theta_1f_1(x, y) + ... + \\theta_{k-1}f_{k-1}(x, y) \\\\\n",
        "\\sigma(z) = \\frac{1}{1 + exp(-z)} \\\\\n",
        "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\\\\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi3fWOaV_oZc",
        "colab_type": "text"
      },
      "source": [
        "Define the `dimension` $k$ of $\\theta$, where $k \\leq 16$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF9K4F3SACLu",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation*}\n",
        "k  = 15\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfCf_eyleFcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9V32Rorcoj0",
        "colab_type": "text"
      },
      "source": [
        "Define the `function` $f_k(x, y)$:\n",
        "\\begin{equation*}\n",
        "f_0(x, y) = -1.5 \\\\\n",
        "f_1(x, y) = (x-0.25)^2, \n",
        "f_2(x, y) = (y-0.25)^2, \\\\\n",
        "f_3(x, y) = x^4 \n",
        "f_4(x, y) = y^4, \\\\ \n",
        "f_5(x, y) = x^6, \n",
        "f_6(x, y) = y^6 \n",
        "f_7(x, y) = x^8, \\\\ \n",
        "f_8(x, y) = y^8, \n",
        "f_9(x, y) = x^{10},\n",
        "f_{10}(x, y) = y^{10}, \\\\\n",
        "f_{11}(x, y) = x^{12}, \n",
        "f_{12}(x, y) = y^{12}, \\\\\n",
        "f_{13}(x, y) = xy, \n",
        "f_{14}(x, y) = x^2y^2\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeAO1d-lAWk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f_k(x, y):\n",
        "    f = []\n",
        "    f.append(-1.5)\n",
        "    f.append((x - 0.25) ** 2)\n",
        "    f.append((y - 0.25) ** 2)\n",
        "    f.append(x ** 4)\n",
        "    f.append(y ** 4)\n",
        "    f.append(x ** 6)\n",
        "    f.append(y ** 6)\n",
        "    f.append(x ** 8)\n",
        "    f.append(y ** 8)\n",
        "    f.append(x ** 10)\n",
        "    f.append(y ** 10)\n",
        "    f.append(x ** 12)\n",
        "    f.append(y ** 12)\n",
        "    f.append(x * y)\n",
        "    f.append((x ** 2) * (y ** 2))\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjlFscOwZFNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = []\n",
        "for i in range(m):\n",
        "    val_k = f_k(x_train[i], y_train[i])\n",
        "    f.append(val_k)\n",
        "# print(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Td7c0f-iHyK",
        "colab_type": "text"
      },
      "source": [
        "Define the `function` $g(x, y; \\theta)$:\n",
        "\\begin{equation}\n",
        "g(x, y; \\theta) = \\theta_0f_0(x, y) + \\theta_1f_1(x, y) + ... + \\theta_{k-1}f_{k-1}(x, y)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnAXhYwBeH2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g(weight, offset):\n",
        "    g = []\n",
        "    for i in range(m):\n",
        "        g_k = offset * f[i][0]\n",
        "        for j in range(k-1):\n",
        "            g_k = g_k + weight[j] * f[i][j + 1]\n",
        "        g.append(g_k)\n",
        "    return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwEIlM7vEbQi",
        "colab_type": "text"
      },
      "source": [
        "Define the following `sigmoid function`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\hat{h} = \\sigma(z), \\text{ where }\n",
        "\\sigma(z) = \\frac{1}{1 + exp(-z)}, \\\\\n",
        "z = g(x, y; \\theta) = \\theta_0f_0(x, y) + \\theta_1f_1(x, y) + ... + \\theta_{k-1}f_{k-1}(x, y) \\\\\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF_EH1XyYihP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic_regression(weight, offset):\n",
        "    y_logistic_regression = []\n",
        "    z = g(weight, offset)\n",
        "    for i in range(m):\n",
        "        # print(z[i])\n",
        "        y_logistic_regression.append(1/(1 + np.exp(-z[i])))\n",
        "        # print(y_logistic_regression[i])        \n",
        "    return y_logistic_regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPA11Brb8dZS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Generate the `Cost Function` with `Gradient Descent` method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3eyeKqM8hIF",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `objective function`\n",
        "\n",
        "Define the following `objective function`.\n",
        "\n",
        "\\begin{equation*}\n",
        "J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^m(-l^{(i)}log(\\sigma(g(x^{(i)}, y^{(i)}; \\theta))) - (1 - l^{(i)})log(1 - \\sigma(g(x^{(i)}, y^{(i)}; \\theta))))\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLaUUmEpBWC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective_function(y_logistic_regression):\n",
        "    error = []\n",
        "    for i in range(m):\n",
        "        error.append((-label[i]) * np.log(y_logistic_regression[i]) - (1 - label[i]) * np.log(1 - y_logistic_regression[i]))\n",
        "    return sum(error) / m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwYNY2pI9XSL",
        "colab_type": "text"
      },
      "source": [
        "### Generate the `gradient descent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6nhoNyHYEV",
        "colab_type": "text"
      },
      "source": [
        "Define the following `derivation`.\n",
        "\\begin{equation}\n",
        "\\frac{\\partial g(x^{(i)}, y^{(i)}; \\theta^{(t)})}{\\partial \\theta_k} = \\frac{\\partial \\theta_0f_0(x, y))}{\\partial \\theta_k} + \\frac{\\partial \\theta_1f_1(x, y))}{\\partial \\theta_k} + ... + \\frac{\\partial \\theta_{k-1}f_{k-1}(x, y))}{\\partial \\theta_k}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAHcEzYzI3rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivation_g(n_data, k):\n",
        "    return f[n_data][k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvHAlub1_KUH",
        "colab_type": "text"
      },
      "source": [
        "Define the `learning rate`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\alpha  = 0.0009\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smtm_inGFG1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0009"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky1NhF8Z9aym",
        "colab_type": "text"
      },
      "source": [
        "Define the following `gradient descent`.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\theta_k^{(t+1)} := \\theta_0^{(t)} - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(\\sigma(g(x^{(i)}, y^{(i)}; \\theta)) - l^{(i)})\\frac{\\partial g(x^{(i)}, y^{(i)}; \\theta^{(t)})}{\\partial \\theta_k}, \\quad\\text{for all $k$} \\\\\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B5bXSBNFK0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(weight, offset, y_logistic_regression):\n",
        "    offset_error = []\n",
        "    weight_error = []\n",
        "    for i in range(k-1):\n",
        "        weight_error.append([])\n",
        "    weightPrime = []\n",
        "    for i in range(m):\n",
        "        regression_error = y_logistic_regression[i] - label[i]\n",
        "        offset_error.append(regression_error * derivation_g(i, 0))\n",
        "        for j in range(k-1):\n",
        "            weight_error[j].append(regression_error * derivation_g(i, j + 1))\n",
        "    offsetPrime = offset - learning_rate * sum(offset_error) / m\n",
        "    for i in range(k-1):\n",
        "        weightPrime.append(weight[i] - learning_rate * sum(weight_error[i]) / m)\n",
        "    return weightPrime, offsetPrime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQ5fNon_luS",
        "colab_type": "text"
      },
      "source": [
        "## 4. `Train` the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmCtCARLAJiS",
        "colab_type": "text"
      },
      "source": [
        "Define the initial `weight`$(\\theta_1^{(0)}, \\theta_2^{(0)}, ..., \\theta_{k-1}^{(0)})$ and `offset`$(\\theta_0^{(0)})$:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\theta_0^{(0)} = 1, \\theta_1^{(0)} = 2, \\theta_2^{(0)} = 3, \\theta_3^{(0)} = 2, \\\\\n",
        "\\theta_4^{(0)} = \\theta_5^{(0)} = \\theta_6^2{(0)} = ... = \\theta_{k-1}^{(0)} = 1.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdrJq6v6Fyg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = []\n",
        "offset = []\n",
        "offset.append(1)\n",
        "weight.append([2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6QhgTxG5Ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost_convergence = []\n",
        "theta_convergence = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKT5lGy2A5V8",
        "colab_type": "text"
      },
      "source": [
        "`Train` the `input data` with the `logistic regression` function above with the `gradient descent`. \\\\\n",
        "Find optimal parameters $\\theta$ using the `traing data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqg-AemPG92m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = m * 200\n",
        "sigma = []\n",
        "cost = []\n",
        "i = 0\n",
        "# print(logistic_regression(weight[i], offset[i]))\n",
        "# print(objective_function(logistic_regression(weight[i], offset[i])))\n",
        "sigma.append(logistic_regression(weight[i], offset[i]))\n",
        "cost.append(objective_function(logistic_regression(weight[i], offset[i])))\n",
        "# print(j)\n",
        "\n",
        "while i < epoch:\n",
        "    i = i + 1\n",
        "\n",
        "#    print(k)\n",
        "\n",
        "    weight.append(gradient_descent(weight[i - 1], offset[i - 1], sigma[i - 1])[0])\n",
        "    offset.append(gradient_descent(weight[i - 1], offset[i - 1], sigma[i - 1])[1])\n",
        "\n",
        "#    print('weight: ', weight)\n",
        "#    print('offset: ', offset)\n",
        "\n",
        "    sigma.append(logistic_regression(weight[i], offset[i]))\n",
        "#    print('sigma: ', sigma)\n",
        "#    print(offset[k])\n",
        "#    print(round(j[k - 1], 2))\n",
        "    cost.append(objective_function(logistic_regression(weight[i], offset[i])))\n",
        "#    print('j: ', j)\n",
        "    if cost[i] == cost[i - 1]:\n",
        "        cost_convergence.append(i)\n",
        "        if offset[i] == offset[i - 1]:\n",
        "            for j in range(15):\n",
        "                if weight[i][j] == weight[i - 1][j]:\n",
        "                    theta_convergence.append(i)\n",
        "\n",
        "theta_convergence.append(epoch)\n",
        "cost_convergence.append(epoch)\n",
        "\n",
        "# print(theta_convergence)\n",
        "# print(cost_convergence)\n",
        "# print('sigma: ', sigma)\n",
        "# print('j: ', j)\n",
        "# print('weight: ', weight)\n",
        "# print('offset: ', offset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB3-M51hBWvL",
        "colab_type": "text"
      },
      "source": [
        "## 5. Compute the `training accuracy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDCv6bcBBg7T",
        "colab_type": "text"
      },
      "source": [
        "Compute the `final training accuracy` in `number (%)`.\n",
        "\\begin{equation}\n",
        "accuracy\\ (\\%) = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}} \\times 100\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd9xOkHaMjLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_minimum = cost.index(min(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz3jJn8j11A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = []\n",
        "i = 0\n",
        "for i in range(epoch):\n",
        "    correct_predictions = 0\n",
        "    for j in range(m):\n",
        "        if round(sigma[i][j], 2) == 0.5:\n",
        "            correct_predictions = correct_predictions + 1\n",
        "        else:\n",
        "            if sigma[i][j] > 0.5:\n",
        "                if label[j] == 0:\n",
        "                    correct_predictions = correct_predictions + 1\n",
        "            else:\n",
        "                if label[j] == 1:\n",
        "                    correct_predictions = correct_predictions + 1\n",
        "    accuracy.append((correct_predictions / m) * 100)\n",
        "# print(index_minimum)\n",
        "index_accuracy_maximum = accuracy.index(max(accuracy))\n",
        "# print(index_accuracy_maximum)\n",
        "# print(accuracy[index_accuracy_maximum])\n",
        "# print(accuracy[index_minimum-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QnL_hZ6xUrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(sigma[0])\n",
        "# print(sigma[1])\n",
        "# print(sigma[2])\n",
        "# print(sigma[index_minimum-1])\n",
        "# print(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnS8zk_SxOWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsekNLQJCU4T",
        "colab_type": "text"
      },
      "source": [
        "## 6. Visualize the `Classifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c79LIbaCaCL",
        "colab_type": "text"
      },
      "source": [
        "Generate the `Classifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLytO3ktSs0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(offset[index_minimum])\n",
        "# print(weight[index_minimum])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMOGxLLFDK1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier(x, y):\n",
        "    val_k = f_k(x, y)\n",
        "    optimal_g = offset[index_accuracy_maximum] * val_k[0]\n",
        "    for i in range(k-1):\n",
        "        optimal_g = optimal_g + weight[index_accuracy_maximum][i] * val_k[i + 1]\n",
        "    return 1 / (1 + np.exp(-optimal_g))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxoFNYszCdwj",
        "colab_type": "text"
      },
      "source": [
        "Visualize the obtained `classifier`, where the `boundary` of the `classifier` is defined by $\\{(x, y) | \\sigma(g(x, y ; \\theta)) = 0.5\\} = \\{(x, y) | g(x, y ; \\theta) = 0\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClQs5nLNvKbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "x = np.arange(-1, 1.25, 0.01)\n",
        "y = np.arange(-1, 1.25, 0.01)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "z = classifier(X, Y)\n",
        "\n",
        "CS = plt.contour(X, Y, z, [1/2], colors='green')\n",
        "CS.clabel()\n",
        "plt.scatter(pointX0, pointY0, c='b')\n",
        "plt.scatter(pointX1, pointY1, c='r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7KprHVQDhY3",
        "colab_type": "text"
      },
      "source": [
        "## 7. **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqRkLfC5Dlpq",
        "colab_type": "text"
      },
      "source": [
        "### 1. **Plot the training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ysZpDTDojS",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training data points` $(x, y)$ with their `labels` $l$ (in `blue` color for `label 0` and `red` color for `label 1`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyyRWqECQaxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(pointX0, pointY0, c='b')\n",
        "plt.scatter(pointX1, pointY1, c='r')\n",
        "plt.tight_layout()\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMDeilIRDuS2",
        "colab_type": "text"
      },
      "source": [
        "### 2. **Write down the high dimensional function $g(x, y; \\theta)$**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUMcS0ltD5i3",
        "colab_type": "text"
      },
      "source": [
        "Write down the `equation` for the `non-linear function` $g(x, y; \\theta)$ used for the `classifier` in `LaTeX` format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdEi-XuUQoKn",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation*}\n",
        "g(x, y; \\theta) = \\theta_0f_0(x, y) + \\theta_1f_1(x, y) + ... + \\theta_{k-1}f_{k-1}(x, y), \\\\\n",
        "\\text{where }\n",
        "f_0(x, y) = - 1.5 \\\\\n",
        "f_1(x, y) = (x-0.25)^2, \n",
        "f_2(x, y) = (y-0.25)^2, \\\\\n",
        "f_3(x, y) = x^4 \n",
        "f_4(x, y) = y^4, \\\\ \n",
        "f_5(x, y) = x^6, \n",
        "f_6(x, y) = y^6 \n",
        "f_7(x, y) = x^8, \\\\ \n",
        "f_8(x, y) = y^8, \n",
        "f_9(x, y) = x^{10}, \n",
        "f_{10}(x, y) = y^{10}, \\\\\n",
        "f_{11}(x, y) = x^{12}, \n",
        "f_{12}(x, y) = y^{12}, \\\\\n",
        "f_{13}(x, y) = xy, \n",
        "f_{14}(x, y) = x^2y^2\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OaWzfYVEHU8",
        "colab_type": "text"
      },
      "source": [
        "### 3. **Plot the training error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiXK5YZWEOa_",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training error` $J(\\theta)$ at `every iteration` of `gradient descent` until `convergence` (in `blue` color)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkLIl-_u6j5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "x_out = np.arange(0, cost_convergence[0])\n",
        "plt.xlabel('t (iteration)')\n",
        "\n",
        "plt.plot(x_out, cost[:cost_convergence[0]], color = 'blue')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-uLMcy8EWVF",
        "colab_type": "text"
      },
      "source": [
        "### 4. **Plot the training accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLHkTd5HEckz",
        "colab_type": "text"
      },
      "source": [
        "Plot the `training accuracy` at `every iteration` of `gradient descent` until `convergence` (in `red` color)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiSDFYg1UCS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "x_out = np.arange(0, index_accuracy_maximum + 20)\n",
        "plt.xlabel('t (iteration)')\n",
        "\n",
        "plt.plot(x_out, accuracy[:index_accuracy_maximum + 20], color = 'red')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxLX3paVEm_j",
        "colab_type": "text"
      },
      "source": [
        "### 5. **Write down the final training accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jU5_K-YEtaA",
        "colab_type": "text"
      },
      "source": [
        "Present the `final training accuracy` in `number (%)` at `convergence`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AIIPc0VUoiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy[index_accuracy_maximum], '(%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk7-PiBME2kR",
        "colab_type": "text"
      },
      "source": [
        "### 6. **Plot the optimal clssifier superimposed on the training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Ipv7ojE-g8",
        "colab_type": "text"
      },
      "source": [
        "Plot the `boundary` of the `optimal classifier` at `convergence` (in `green` color). \\\\\n",
        "The `boundary` of the `classifier` is defined by $\\{ (x, y) \\mid \\sigma(g(x, y ; \\theta)) = 0.5 \\} = \\{ (x, y) \\mid g(x, y ; \\theta) = 0 \\}$. \\\\\n",
        "Plot the `training data points` $(x, y)$ with their `labels` $l$ superimposed on the illustration of the `classifier` using `contour` function in `python3`(in `blue` color for `label 0` and `red` color for `label 1`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRVtoo8wVVcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "x = np.arange(-1, 1.25, 0.01)\n",
        "y = np.arange(-1, 1.25, 0.01)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "z = classifier(X, Y)\n",
        "\n",
        "CS = plt.contour(X, Y, z, [1/2], colors='green')\n",
        "plt.scatter(pointX0, pointY0, c='b')\n",
        "plt.scatter(pointX1, pointY1, c='r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}